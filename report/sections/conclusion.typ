= Conclusion
The results of these experiments are positive, albeit still quite improvable. For instance, the training set could be expanded to include more diverse instances, which would likely help the models generalize better.
Solving larger instances, such as the whole MIPLIB benchmark, would provide a more comprehensive evaluation of the trained estimators. 

Moreover, hyperparameter tuning could be refined further to verify whether it's possible to achieve better performances.

Finally, it could be interesting to explore hybrid branching strategies. These aim to combine the advantages of different approaches, for instance, the computational efficiency of Pseudo Cost branching with the predictive accuracy of the learned SB. Such a combination could potentially yield better results than the ones presented in this report.
